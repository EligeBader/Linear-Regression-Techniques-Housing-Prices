{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uPu56RCks4W"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.1 in c:\\users\\elige\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\elige\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\elige\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.1) (1.15.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\elige\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\elige\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.1) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install scikit-learn==1.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ozewf7LriipC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import TargetEncoder, LabelEncoder, OrdinalEncoder, MinMaxScaler, OneHotEncoder, power_transform, PowerTransformer\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy.stats import yeojohnson\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "import dill\n",
        "import os\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from define_function import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjPgeK6ikwKO"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Py7mAkw3iipD"
      },
      "outputs": [],
      "source": [
        "df = load_data('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XaOodrMEiipD",
        "outputId": "362923ae-8ff4-4754-fafc-55056403e060"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>1456</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7917</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>1457</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13175</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>1458</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>66.0</td>\n",
              "      <td>9042</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GdPrv</td>\n",
              "      <td>Shed</td>\n",
              "      <td>2500</td>\n",
              "      <td>5</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>1459</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9717</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>142125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>1460</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9937</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>147500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
              "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
              "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
              "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
              "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
              "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
              "\n",
              "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
              "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
              "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
              "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
              "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "\n",
              "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0         2   2008        WD         Normal     208500  \n",
              "1         5   2007        WD         Normal     181500  \n",
              "2         9   2008        WD         Normal     223500  \n",
              "3         2   2006        WD        Abnorml     140000  \n",
              "4        12   2008        WD         Normal     250000  \n",
              "...     ...    ...       ...            ...        ...  \n",
              "1455      8   2007        WD         Normal     175000  \n",
              "1456      2   2010        WD         Normal     210000  \n",
              "1457      5   2010        WD         Normal     266500  \n",
              "1458      4   2010        WD         Normal     142125  \n",
              "1459      6   2008        WD         Normal     147500  \n",
              "\n",
              "[1460 rows x 81 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R_xFnIZkyuh"
      },
      "source": [
        "# Drop Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KN8ZeFQEiipE"
      },
      "outputs": [],
      "source": [
        "df = drop_features(df, features_to_drop=['Alley','PoolQC','Fence','MiscFeature', ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iH1GC3xk1ti"
      },
      "source": [
        "# Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h5khM9IciipE"
      },
      "outputs": [],
      "source": [
        "df = clean_data(df, target='SalePrice')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFX9k3JPiipE",
        "outputId": "caf2b14c-3fe5-49d9-93e6-6782c6268b23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL36trJ9k3Al"
      },
      "source": [
        "# Encode Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZVnPwUY3iipE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "Target_Encoding_list = ['MSZoning', 'Street', 'Utilities', 'LotConfig', 'Neighborhood', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'GarageType', 'SaleType']\n",
        "Ordinal_Encoding_list= ['LotShape', 'LandContour', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleCondition']\n",
        "\n",
        "encoding_methods = {col: 'target' for col in Target_Encoding_list}\n",
        "encoding_methods.update({col: 'ordinal' for col in Ordinal_Encoding_list}) # merge the dict with the target dict\n",
        "\n",
        "df = encode_data(df, encoding_methods, train=True, target =['SalePrice'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1uBfDmJiipE",
        "outputId": "1f82d8d9-c1fa-44eb-8c5c-0a69260113e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if al categorical were encoded\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Oyl0zAVhiipE",
        "outputId": "58f3fc91-2a81-49c5-ec8c-3b391417977d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>...</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>177972.952552</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>181620.429673</td>\n",
              "      <td>...</td>\n",
              "      <td>272.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>177972.952552</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>1456.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7917.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>1457.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13175.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>1458.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>66.0</td>\n",
              "      <td>9042.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>1459.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9717.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>142125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>1460.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>190995.949254</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9937.0</td>\n",
              "      <td>181130.394622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180950.936426</td>\n",
              "      <td>176941.547778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>173406.590852</td>\n",
              "      <td>4.0</td>\n",
              "      <td>147500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 77 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id  MSSubClass       MSZoning  LotFrontage  LotArea         Street  \\\n",
              "0        1.0        60.0  190995.949254         65.0   8450.0  181130.394622   \n",
              "1        2.0        20.0  190995.949254         80.0   9600.0  181130.394622   \n",
              "2        3.0        60.0  190995.949254         68.0  11250.0  181130.394622   \n",
              "3        4.0        70.0  190995.949254         60.0   9550.0  181130.394622   \n",
              "4        5.0        60.0  190995.949254         84.0  14260.0  181130.394622   \n",
              "...      ...         ...            ...          ...      ...            ...   \n",
              "1455  1456.0        60.0  190995.949254         62.0   7917.0  181130.394622   \n",
              "1456  1457.0        20.0  190995.949254         85.0  13175.0  181130.394622   \n",
              "1457  1458.0        70.0  190995.949254         66.0   9042.0  181130.394622   \n",
              "1458  1459.0        20.0  190995.949254         68.0   9717.0  181130.394622   \n",
              "1459  1460.0        20.0  190995.949254         75.0   9937.0  181130.394622   \n",
              "\n",
              "      LotShape  LandContour      Utilities      LotConfig  ...  EnclosedPorch  \\\n",
              "0          3.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "1          3.0          3.0  180950.936426  177972.952552  ...            0.0   \n",
              "2          0.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "3          0.0          3.0  180950.936426  181620.429673  ...          272.0   \n",
              "4          0.0          3.0  180950.936426  177972.952552  ...            0.0   \n",
              "...        ...          ...            ...            ...  ...            ...   \n",
              "1455       3.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "1456       3.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "1457       3.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "1458       3.0          3.0  180950.936426  176941.547778  ...          112.0   \n",
              "1459       3.0          3.0  180950.936426  176941.547778  ...            0.0   \n",
              "\n",
              "      3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
              "0           0.0          0.0       0.0      0.0     2.0  2008.0   \n",
              "1           0.0          0.0       0.0      0.0     5.0  2007.0   \n",
              "2           0.0          0.0       0.0      0.0     9.0  2008.0   \n",
              "3           0.0          0.0       0.0      0.0     2.0  2006.0   \n",
              "4           0.0          0.0       0.0      0.0    12.0  2008.0   \n",
              "...         ...          ...       ...      ...     ...     ...   \n",
              "1455        0.0          0.0       0.0      0.0     8.0  2007.0   \n",
              "1456        0.0          0.0       0.0      0.0     2.0  2010.0   \n",
              "1457        0.0          0.0       0.0   2500.0     5.0  2010.0   \n",
              "1458        0.0          0.0       0.0      0.0     4.0  2010.0   \n",
              "1459        0.0          0.0       0.0      0.0     6.0  2008.0   \n",
              "\n",
              "           SaleType  SaleCondition  SalePrice  \n",
              "0     173406.590852            4.0     208500  \n",
              "1     173406.590852            4.0     181500  \n",
              "2     173406.590852            4.0     223500  \n",
              "3     173406.590852            0.0     140000  \n",
              "4     173406.590852            4.0     250000  \n",
              "...             ...            ...        ...  \n",
              "1455  173406.590852            4.0     175000  \n",
              "1456  173406.590852            4.0     210000  \n",
              "1457  173406.590852            4.0     266500  \n",
              "1458  173406.590852            4.0     142125  \n",
              "1459  173406.590852            4.0     147500  \n",
              "\n",
              "[1460 rows x 77 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4itWpzSnk6e4"
      },
      "source": [
        "# Transform Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PCqx69gYiipF"
      },
      "outputs": [],
      "source": [
        "df = transform_data(df, 'SalePrice')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RFngN_yDqJEb",
        "outputId": "2dc4a0cb-1679-4301-c5ca-9847655ed025"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       0.571154\n",
              "1       0.227627\n",
              "2       0.741868\n",
              "3      -0.425386\n",
              "4       1.015293\n",
              "          ...   \n",
              "1455    0.136679\n",
              "1456    0.588811\n",
              "1457    1.170202\n",
              "1458   -0.387136\n",
              "1459   -0.293072\n",
              "Name: transform_target, Length: 1460, dtype: float64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.transform_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srYbhPeKk_f9"
      },
      "source": [
        "# Train_Test_Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xtjDZa0hiipF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = split_data(df, target ='transform_target', col_dropped = ['SalePrice','Id'], feature_selected= None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9xsmtejlDp9"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1dtoGkGpiipF"
      },
      "outputs": [],
      "source": [
        "model_lr = train_model(LinearRegression, xtrain=X_train, ytrain=y_train)\n",
        "\n",
        "with open('trained_model_LR.pickle', 'wb') as f:\n",
        "    dill.dump(model_lr, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid_xg = {\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.1, 0.05, 0.01],\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'gamma': [0, 0.1, 0.5],\n",
        "    'subsample': [0.5, 0.8, 1]\n",
        "}\n",
        "\n",
        "\n",
        "model_XG = train_model(XGBRegressor, xtrain=X_train, ytrain=y_train, param_grid=param_grid_xg, best_combination=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('trained_model_XG.pickle', 'wb') as f:\n",
        "    dill.dump(model_XG, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "60RIM7bTiipF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
            "1440 fits failed out of a total of 4320.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "972 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "468 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\elige\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.74389774 0.74765069 0.74414516 0.74427528 0.74366853 0.74271819\n",
            " 0.74454314 0.74867545 0.73926685 0.73896269 0.74562749 0.74765096\n",
            " 0.74301865 0.74416713 0.74187403 0.74531013 0.73976925 0.74437759\n",
            " 0.74512222 0.74482966 0.74443512 0.74432715 0.74569999 0.74571701\n",
            " 0.73877106 0.7422943  0.74483888 0.74423046 0.74933372 0.74619042\n",
            " 0.74694772 0.74346871 0.74194816 0.74558884 0.74573723 0.7450999\n",
            " 0.71418528 0.72703592 0.73110173 0.7294993  0.72665806 0.72929256\n",
            " 0.72636338 0.72536626 0.72163673 0.72787346 0.72612833 0.72898275\n",
            " 0.73279306 0.72881524 0.7240019  0.72776897 0.73196451 0.72543856\n",
            " 0.72569805 0.7293799  0.72612623 0.72630281 0.72559271 0.73106437\n",
            " 0.72368899 0.72901951 0.73102092 0.7312046  0.72456283 0.72365881\n",
            " 0.7269543  0.72998948 0.72185959 0.72640221 0.73183293 0.7261076\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.81680617 0.81834698 0.82211951 0.82174134 0.82184724 0.81929787\n",
            " 0.82195498 0.82037513 0.82332515 0.8177763  0.82111801 0.82087451\n",
            " 0.82262236 0.82138913 0.82334347 0.82355213 0.82128568 0.82210119\n",
            " 0.82315884 0.82132318 0.81770009 0.82129417 0.82235793 0.8228941\n",
            " 0.81706196 0.8234822  0.82150035 0.82137355 0.81899636 0.82177049\n",
            " 0.81979261 0.82218243 0.82112567 0.82054988 0.81985781 0.82253836\n",
            " 0.80530386 0.80723656 0.80794964 0.81046705 0.80753886 0.80825263\n",
            " 0.80999103 0.81008838 0.80660847 0.80561136 0.80960414 0.80966394\n",
            " 0.80754318 0.81301225 0.81030333 0.81031382 0.8075793  0.80661785\n",
            " 0.81226398 0.81081449 0.81435881 0.81003131 0.81378064 0.809586\n",
            " 0.80843552 0.80862618 0.81083991 0.81160973 0.81114421 0.80860418\n",
            " 0.8114063  0.81135476 0.80134071 0.80746028 0.80948593 0.81001548\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.84348304 0.84893433 0.85078512 0.85159738 0.851511   0.84833403\n",
            " 0.85144943 0.85267186 0.84612448 0.84824335 0.85034336 0.85039692\n",
            " 0.8503201  0.84979653 0.85278113 0.85111957 0.84524748 0.85004554\n",
            " 0.85093004 0.85035488 0.84621532 0.84766554 0.85016674 0.84958058\n",
            " 0.84373684 0.84949708 0.84823145 0.84899688 0.84655701 0.84909399\n",
            " 0.84846014 0.84705584 0.84314819 0.84619327 0.84515293 0.84743601\n",
            " 0.84385971 0.84283525 0.84483305 0.84506835 0.84008174 0.84216746\n",
            " 0.84333261 0.84335665 0.83364027 0.83885958 0.8415343  0.84162686\n",
            " 0.83897507 0.84396362 0.84192341 0.84320862 0.84079318 0.84269255\n",
            " 0.84189896 0.84218058 0.84009765 0.84044914 0.84269469 0.84131787\n",
            " 0.83706781 0.83704813 0.83820225 0.8390369  0.83440247 0.84113961\n",
            " 0.83898793 0.8390558  0.83627365 0.83706549 0.83659025 0.83960177\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.85917125 0.86357539 0.8634195  0.86122379 0.85423198 0.86276956\n",
            " 0.86189894 0.86160923 0.85330661 0.85940903 0.85946083 0.85861001\n",
            " 0.86029402 0.86016759 0.86047096 0.8617823  0.85588478 0.86193004\n",
            " 0.86107396 0.8605544  0.85331996 0.85743312 0.85653858 0.85945492\n",
            " 0.8526235  0.85226504 0.85541745 0.85676459 0.85209129 0.85412071\n",
            " 0.85483692 0.85511096 0.85190043 0.85261972 0.85514924 0.85493442\n",
            " 0.85291044 0.85910827 0.8583581  0.85712308 0.85604186 0.8537148\n",
            " 0.85541047 0.85710143 0.84915063 0.8521435  0.85231469 0.85327593\n",
            " 0.8531841  0.85233902 0.8565314  0.85715342 0.85494397 0.85449453\n",
            " 0.8557198  0.85583217 0.84955489 0.84897044 0.85091661 0.85105291\n",
            " 0.84640078 0.84363717 0.85037467 0.84959215 0.84070136 0.847646\n",
            " 0.84871917 0.84867935 0.84410653 0.84480914 0.84721348 0.84697629\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.74008646 0.74597546 0.74185475 0.74561127 0.74443073 0.74300539\n",
            " 0.74463789 0.74370413 0.74190994 0.74498524 0.7427588  0.74450322\n",
            " 0.74060266 0.74195917 0.74590753 0.7446168  0.74011484 0.74466327\n",
            " 0.74569677 0.74377538 0.74103783 0.74211285 0.74430005 0.74519216\n",
            " 0.73983115 0.74427572 0.74626728 0.74573499 0.7427861  0.74336002\n",
            " 0.74288427 0.7465262  0.74142663 0.74509076 0.74559171 0.74527059\n",
            " 0.72465509 0.72926101 0.72718681 0.72876048 0.72573141 0.72441003\n",
            " 0.72844917 0.725211   0.7185608  0.73213799 0.72734159 0.72765419\n",
            " 0.72411201 0.72292009 0.7283888  0.7281902  0.72797749 0.7287298\n",
            " 0.729975   0.72581395 0.72449519 0.72781389 0.72747433 0.72912847\n",
            " 0.72920741 0.72425017 0.72626767 0.72925916 0.72516749 0.73038849\n",
            " 0.72379944 0.72838554 0.72427585 0.72490032 0.72772445 0.72626759\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.82592956 0.82416901 0.82485118 0.82419704 0.82074333 0.8226681\n",
            " 0.82477194 0.82361725 0.82222462 0.82753277 0.82316392 0.82317176\n",
            " 0.82392013 0.82076287 0.82520179 0.82591863 0.82079207 0.82413091\n",
            " 0.82308169 0.82315174 0.82598541 0.82388137 0.82471508 0.82363496\n",
            " 0.82233354 0.82137931 0.82447075 0.82594165 0.8235968  0.82601387\n",
            " 0.82469927 0.82481521 0.82241582 0.82532361 0.82337445 0.82471405\n",
            " 0.81146916 0.81486063 0.81340025 0.81293    0.81113029 0.81425934\n",
            " 0.81387401 0.81334187 0.81034157 0.80968661 0.81074115 0.81287162\n",
            " 0.81354993 0.81178073 0.81403633 0.81300774 0.80916317 0.81399259\n",
            " 0.81315082 0.81473017 0.81049554 0.80999346 0.81567368 0.81442819\n",
            " 0.81054142 0.81152129 0.81206571 0.81243192 0.81292212 0.81335308\n",
            " 0.81559286 0.81281633 0.80762372 0.81192223 0.81296517 0.81350403\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.85231771 0.85519692 0.85671107 0.8551968  0.8542916  0.85752623\n",
            " 0.8562073  0.85560619 0.8514253  0.85354744 0.85450313 0.85446774\n",
            " 0.85224608 0.85385858 0.85566086 0.856468   0.85334597 0.85414634\n",
            " 0.85739336 0.85501133 0.85254517 0.85235636 0.85707071 0.85469831\n",
            " 0.84989362 0.85580759 0.85322862 0.85461995 0.85040032 0.85356059\n",
            " 0.85483056 0.85423807 0.85234371 0.85396768 0.85420953 0.85338337\n",
            " 0.84649066 0.84713174 0.84934999 0.84976822 0.8426725  0.8479913\n",
            " 0.8499573  0.84971349 0.84855419 0.84721609 0.84547813 0.8483022\n",
            " 0.84703985 0.84641456 0.84852491 0.8489816  0.84879775 0.8468788\n",
            " 0.84842068 0.8489191  0.84260709 0.84702202 0.84790498 0.84726383\n",
            " 0.84327074 0.84553248 0.84587986 0.84664854 0.84002747 0.845927\n",
            " 0.84640243 0.84706207 0.84629081 0.84571539 0.84624726 0.84537122\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.86381564 0.86768355 0.86947318 0.86892505 0.86151867 0.86818115\n",
            " 0.86933488 0.86787882 0.86314086 0.86283776 0.86422676 0.86663761\n",
            " 0.86382642 0.86895544 0.86885122 0.86895541 0.86616495 0.86777967\n",
            " 0.86807196 0.86818278 0.86320074 0.8648367  0.8662228  0.86669481\n",
            " 0.8613781  0.86280388 0.86341649 0.86482532 0.860397   0.86494856\n",
            " 0.86359519 0.86406581 0.85841337 0.86194275 0.86371348 0.86303387\n",
            " 0.86091301 0.86249984 0.86417038 0.86262896 0.85762601 0.86420813\n",
            " 0.86346525 0.86356356 0.86096086 0.85941461 0.86040448 0.86049206\n",
            " 0.85825924 0.86070798 0.86381752 0.8629988  0.85714527 0.86172574\n",
            " 0.86157988 0.86413312 0.85867983 0.85799192 0.86015889 0.85970453\n",
            " 0.85385877 0.85609075 0.85733967 0.8577152  0.85100237 0.85588148\n",
            " 0.85835501 0.85731159 0.8554044  0.85516853 0.85466225 0.85655968]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "\n",
        "model_rf = train_model(RandomForestRegressor, xtrain=X_train, ytrain=y_train, param_grid=param_grid_rf, best_combination=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('trained_model_rf.pickle', 'wb') as f:\n",
        "    dill.dump(model_rf, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-sjNXBNGiipF"
      },
      "outputs": [],
      "source": [
        "with open('feature_list.pickle', 'wb') as f:\n",
        "    dill.dump(X_train.columns.tolist(), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd59tuLglITg"
      },
      "source": [
        "# Build Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data without transformation\n",
        "X_train_nn, X_test_nn, y_train_nn, y_test_nn = split_data(df, target ='SalePrice', col_dropped = ['transform_target','Id'], feature_selected= None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1168, 75)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_nn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1168,)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_nn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVdOn8PYlKbN",
        "outputId": "4602c8e2-ffc1-4e46-89c6-c39aaaacf1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13646707712.0000 - mse: 13646707712.0000 - val_loss: 7254137344.0000 - val_mse: 7254137344.0000\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6631764480.0000 - mse: 6631764480.0000 - val_loss: 4200463104.0000 - val_mse: 4200463104.0000\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4317946368.0000 - mse: 4317946368.0000 - val_loss: 3559481344.0000 - val_mse: 3559481344.0000\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4635144192.0000 - mse: 4635144192.0000 - val_loss: 3461703936.0000 - val_mse: 3461703936.0000\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4527638016.0000 - mse: 4527638016.0000 - val_loss: 3349314048.0000 - val_mse: 3349314048.0000\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4370728448.0000 - mse: 4370728448.0000 - val_loss: 3213877760.0000 - val_mse: 3213877760.0000\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3403463680.0000 - mse: 3403463680.0000 - val_loss: 3046594560.0000 - val_mse: 3046594560.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4662004224.0000 - mse: 4662004224.0000 - val_loss: 2855544064.0000 - val_mse: 2855544064.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3361564928.0000 - mse: 3361564928.0000 - val_loss: 2685552640.0000 - val_mse: 2685552640.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2860904192.0000 - mse: 2860904192.0000 - val_loss: 2597907968.0000 - val_mse: 2597907968.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2705908992.0000 - mse: 2705908992.0000 - val_loss: 2505619968.0000 - val_mse: 2505619968.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3410304768.0000 - mse: 3410304768.0000 - val_loss: 2440930304.0000 - val_mse: 2440930304.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2874407936.0000 - mse: 2874407936.0000 - val_loss: 2388997632.0000 - val_mse: 2388997632.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2683843328.0000 - mse: 2683843328.0000 - val_loss: 2348337664.0000 - val_mse: 2348337664.0000\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2443058688.0000 - mse: 2443058688.0000 - val_loss: 2306372864.0000 - val_mse: 2306372864.0000\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2686044416.0000 - mse: 2686044416.0000 - val_loss: 2287786752.0000 - val_mse: 2287786752.0000\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2912721664.0000 - mse: 2912721664.0000 - val_loss: 2247766016.0000 - val_mse: 2247766016.0000\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2542557184.0000 - mse: 2542557184.0000 - val_loss: 2225840896.0000 - val_mse: 2225840896.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2499675392.0000 - mse: 2499675392.0000 - val_loss: 2219909120.0000 - val_mse: 2219909120.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3102367488.0000 - mse: 3102367488.0000 - val_loss: 2194464768.0000 - val_mse: 2194464768.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2563701248.0000 - mse: 2563701248.0000 - val_loss: 2182304512.0000 - val_mse: 2182304512.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2454248704.0000 - mse: 2454248704.0000 - val_loss: 2188657152.0000 - val_mse: 2188657152.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2459161344.0000 - mse: 2459161344.0000 - val_loss: 2167922176.0000 - val_mse: 2167922176.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2140807936.0000 - mse: 2140807936.0000 - val_loss: 2160086784.0000 - val_mse: 2160086784.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2484624640.0000 - mse: 2484624640.0000 - val_loss: 2156894208.0000 - val_mse: 2156894208.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2546295552.0000 - mse: 2546295552.0000 - val_loss: 2154314496.0000 - val_mse: 2154314496.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2561376512.0000 - mse: 2561376512.0000 - val_loss: 2148885504.0000 - val_mse: 2148885504.0000\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2264736768.0000 - mse: 2264736768.0000 - val_loss: 2152367616.0000 - val_mse: 2152367616.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2527794432.0000 - mse: 2527794432.0000 - val_loss: 2142963456.0000 - val_mse: 2142963456.0000\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2160985088.0000 - mse: 2160985088.0000 - val_loss: 2138441216.0000 - val_mse: 2138441216.0000\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2321207040.0000 - mse: 2321207040.0000 - val_loss: 2193966592.0000 - val_mse: 2193966592.0000\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2200500480.0000 - mse: 2200500480.0000 - val_loss: 2150162688.0000 - val_mse: 2150162688.0000\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2736524800.0000 - mse: 2736524800.0000 - val_loss: 2131172096.0000 - val_mse: 2131172096.0000\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2557600512.0000 - mse: 2557600512.0000 - val_loss: 2144251008.0000 - val_mse: 2144251008.0000\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2680862464.0000 - mse: 2680862464.0000 - val_loss: 2126844160.0000 - val_mse: 2126844160.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2100131328.0000 - mse: 2100131328.0000 - val_loss: 2154069504.0000 - val_mse: 2154069504.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2219779072.0000 - mse: 2219779072.0000 - val_loss: 2125869312.0000 - val_mse: 2125869312.0000\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2332338176.0000 - mse: 2332338176.0000 - val_loss: 2171877632.0000 - val_mse: 2171877632.0000\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2805738496.0000 - mse: 2805738496.0000 - val_loss: 2121084800.0000 - val_mse: 2121084800.0000\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2178051328.0000 - mse: 2178051328.0000 - val_loss: 2111221376.0000 - val_mse: 2111221376.0000\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2208153600.0000 - mse: 2208153600.0000 - val_loss: 2102884992.0000 - val_mse: 2102884992.0000\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2266135040.0000 - mse: 2266135040.0000 - val_loss: 2101667712.0000 - val_mse: 2101667712.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2308288256.0000 - mse: 2308288256.0000 - val_loss: 2113451264.0000 - val_mse: 2113451264.0000\n",
            "Epoch 44/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1830387328.0000 - mse: 1830387328.0000 - val_loss: 2094306176.0000 - val_mse: 2094306176.0000\n",
            "Epoch 45/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2127420160.0000 - mse: 2127420160.0000 - val_loss: 2087452544.0000 - val_mse: 2087452544.0000\n",
            "Epoch 46/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2593034752.0000 - mse: 2593034752.0000 - val_loss: 2101919104.0000 - val_mse: 2101919104.0000\n",
            "Epoch 47/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3094381056.0000 - mse: 3094381056.0000 - val_loss: 2089790336.0000 - val_mse: 2089790336.0000\n",
            "Epoch 48/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2062131456.0000 - mse: 2062131456.0000 - val_loss: 2083812864.0000 - val_mse: 2083812864.0000\n",
            "Epoch 49/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2191590400.0000 - mse: 2191590400.0000 - val_loss: 2091320832.0000 - val_mse: 2091320832.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3155312384.0000 - mse: 3155312384.0000 - val_loss: 2081858432.0000 - val_mse: 2081858432.0000\n",
            "Epoch 51/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2276834304.0000 - mse: 2276834304.0000 - val_loss: 2082912256.0000 - val_mse: 2082912256.0000\n",
            "Epoch 52/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2693051136.0000 - mse: 2693051136.0000 - val_loss: 2078775424.0000 - val_mse: 2078775424.0000\n",
            "Epoch 53/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2685511680.0000 - mse: 2685511680.0000 - val_loss: 2074556416.0000 - val_mse: 2074556416.0000\n",
            "Epoch 54/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2523412224.0000 - mse: 2523412224.0000 - val_loss: 2138753408.0000 - val_mse: 2138753408.0000\n",
            "Epoch 55/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2130128000.0000 - mse: 2130128000.0000 - val_loss: 2064407808.0000 - val_mse: 2064407808.0000\n",
            "Epoch 56/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2081477504.0000 - mse: 2081477504.0000 - val_loss: 2064491008.0000 - val_mse: 2064491008.0000\n",
            "Epoch 57/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2479223296.0000 - mse: 2479223296.0000 - val_loss: 2113619200.0000 - val_mse: 2113619200.0000\n",
            "Epoch 58/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1869020672.0000 - mse: 1869020672.0000 - val_loss: 2055274112.0000 - val_mse: 2055274112.0000\n",
            "Epoch 59/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2487793152.0000 - mse: 2487793152.0000 - val_loss: 2058825728.0000 - val_mse: 2058825728.0000\n",
            "Epoch 60/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2419241984.0000 - mse: 2419241984.0000 - val_loss: 2098442112.0000 - val_mse: 2098442112.0000\n",
            "Epoch 61/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2394732288.0000 - mse: 2394732288.0000 - val_loss: 2047760384.0000 - val_mse: 2047760384.0000\n",
            "Epoch 62/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2377772800.0000 - mse: 2377772800.0000 - val_loss: 2041000704.0000 - val_mse: 2041000704.0000\n",
            "Epoch 63/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2406906624.0000 - mse: 2406906624.0000 - val_loss: 2040241408.0000 - val_mse: 2040241408.0000\n",
            "Epoch 64/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1993795456.0000 - mse: 1993795456.0000 - val_loss: 2056228864.0000 - val_mse: 2056228864.0000\n",
            "Epoch 65/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1854327936.0000 - mse: 1854327936.0000 - val_loss: 2057058560.0000 - val_mse: 2057058560.0000\n",
            "Epoch 66/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2206947840.0000 - mse: 2206947840.0000 - val_loss: 2033679360.0000 - val_mse: 2033679360.0000\n",
            "Epoch 67/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2153593856.0000 - mse: 2153593856.0000 - val_loss: 2037424512.0000 - val_mse: 2037424512.0000\n",
            "Epoch 68/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2288725504.0000 - mse: 2288725504.0000 - val_loss: 2042992768.0000 - val_mse: 2042992768.0000\n",
            "Epoch 69/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2199537152.0000 - mse: 2199537152.0000 - val_loss: 2076149760.0000 - val_mse: 2076149760.0000\n",
            "Epoch 70/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2036969984.0000 - mse: 2036969984.0000 - val_loss: 2027089920.0000 - val_mse: 2027089920.0000\n",
            "Epoch 71/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2051092224.0000 - mse: 2051092224.0000 - val_loss: 2030252672.0000 - val_mse: 2030252672.0000\n",
            "Epoch 72/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2170124032.0000 - mse: 2170124032.0000 - val_loss: 2026886784.0000 - val_mse: 2026886784.0000\n",
            "Epoch 73/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2345905664.0000 - mse: 2345905664.0000 - val_loss: 2031250688.0000 - val_mse: 2031250688.0000\n",
            "Epoch 74/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1932821760.0000 - mse: 1932821760.0000 - val_loss: 2053152000.0000 - val_mse: 2053152000.0000\n",
            "Epoch 75/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2380151552.0000 - mse: 2380151552.0000 - val_loss: 2038576640.0000 - val_mse: 2038576640.0000\n",
            "Epoch 76/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2067570560.0000 - mse: 2067570560.0000 - val_loss: 2012098560.0000 - val_mse: 2012098560.0000\n",
            "Epoch 77/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2057086080.0000 - mse: 2057086080.0000 - val_loss: 2016273792.0000 - val_mse: 2016273792.0000\n",
            "Epoch 78/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2409554176.0000 - mse: 2409554176.0000 - val_loss: 2021254272.0000 - val_mse: 2021254272.0000\n",
            "Epoch 79/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1917802496.0000 - mse: 1917802496.0000 - val_loss: 2053859968.0000 - val_mse: 2053859968.0000\n",
            "Epoch 80/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2233203456.0000 - mse: 2233203456.0000 - val_loss: 2006925568.0000 - val_mse: 2006925568.0000\n",
            "Epoch 81/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2039392512.0000 - mse: 2039392512.0000 - val_loss: 2012273920.0000 - val_mse: 2012273920.0000\n",
            "Epoch 82/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2011566080.0000 - mse: 2011566080.0000 - val_loss: 2102980224.0000 - val_mse: 2102980224.0000\n",
            "Epoch 83/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2784546048.0000 - mse: 2784546048.0000 - val_loss: 2005942400.0000 - val_mse: 2005942400.0000\n",
            "Epoch 84/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2821051904.0000 - mse: 2821051904.0000 - val_loss: 2001493120.0000 - val_mse: 2001493120.0000\n",
            "Epoch 85/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2144906240.0000 - mse: 2144906240.0000 - val_loss: 1996130944.0000 - val_mse: 1996130944.0000\n",
            "Epoch 86/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2138310016.0000 - mse: 2138310016.0000 - val_loss: 1991187456.0000 - val_mse: 1991187456.0000\n",
            "Epoch 87/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2255334912.0000 - mse: 2255334912.0000 - val_loss: 1996633600.0000 - val_mse: 1996633600.0000\n",
            "Epoch 88/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2157132800.0000 - mse: 2157132800.0000 - val_loss: 1998724352.0000 - val_mse: 1998724352.0000\n",
            "Epoch 89/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2856607232.0000 - mse: 2856607232.0000 - val_loss: 1998556160.0000 - val_mse: 1998556160.0000\n",
            "Epoch 90/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2083502976.0000 - mse: 2083502976.0000 - val_loss: 2015487872.0000 - val_mse: 2015487872.0000\n",
            "Epoch 91/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1953385088.0000 - mse: 1953385088.0000 - val_loss: 1999994880.0000 - val_mse: 1999994880.0000\n",
            "Epoch 92/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1955157504.0000 - mse: 1955157504.0000 - val_loss: 2036665344.0000 - val_mse: 2036665344.0000\n",
            "Epoch 93/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2357822976.0000 - mse: 2357822976.0000 - val_loss: 1992120704.0000 - val_mse: 1992120704.0000\n",
            "Epoch 94/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2177982208.0000 - mse: 2177982208.0000 - val_loss: 1980791168.0000 - val_mse: 1980791168.0000\n",
            "Epoch 95/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2279174912.0000 - mse: 2279174912.0000 - val_loss: 1979432576.0000 - val_mse: 1979432576.0000\n",
            "Epoch 96/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2078482688.0000 - mse: 2078482688.0000 - val_loss: 1979701888.0000 - val_mse: 1979701888.0000\n",
            "Epoch 97/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1962624256.0000 - mse: 1962624256.0000 - val_loss: 1973662208.0000 - val_mse: 1973662208.0000\n",
            "Epoch 98/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2056227200.0000 - mse: 2056227200.0000 - val_loss: 1979458816.0000 - val_mse: 1979458816.0000\n",
            "Epoch 99/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2619870720.0000 - mse: 2619870720.0000 - val_loss: 2004712064.0000 - val_mse: 2004712064.0000\n",
            "Epoch 100/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2111167872.0000 - mse: 2111167872.0000 - val_loss: 1979942528.0000 - val_mse: 1979942528.0000\n"
          ]
        }
      ],
      "source": [
        "model_nn, history = neural_network_model(X=X_train_nn, y=y_train_nn, loss='mse', metrics='mse', activations='relu', widths=[64], num_layers=2, epochs=100, learning_rate=0.0001, validation_split = 0.3333)\n",
        "\n",
        "\n",
        "with open('trained_nn_model.pickle', 'wb') as f:\n",
        "    dill.dump(model_nn, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,269</span> (106.52 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,269\u001b[0m (106.52 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,089</span> (35.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,089\u001b[0m (35.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,180</span> (71.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m18,180\u001b[0m (71.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_nn.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1uPu56RCks4W",
        "xjPgeK6ikwKO",
        "_R_xFnIZkyuh",
        "1iH1GC3xk1ti",
        "IL36trJ9k3Al",
        "4itWpzSnk6e4",
        "srYbhPeKk_f9",
        "B9xsmtejlDp9",
        "kd59tuLglITg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
